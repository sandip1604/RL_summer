import numpy as np
import random
import Gridenv
import copy
import matplotlib.pyplot as plt

env = Gridenv.Grid()
###----------------- random steps taken-----------------------------####

# no_episodes = 1000
# env = Gridenv.Grid()
# list_steps = []
# for i in range(no_episodes):
#     terminal_state = 0
#     steps = 0
#     env.reset()
#
#     while True:
#         action = random.randint(0, 3)
#         list_1 = env.step(action)
#         steps = steps + 1
#         # print(steps)
#         # print(list_1)
#         # print(list_1[0])
#         if list_1[3] == 1:
#             print(i)
#         if list_1[4] == 1:
#             break
#     # print("-----------------------")
#     list_steps.append(steps)
#
# #plt.plot(list_steps)
# #plt.ylabel("number of steps per episodes")
# #plt.show()

##---------------------------Tabular Q-learning  ----------------------------##
learning_rate = 0.2
discount_rate = 0.9
epsilon1 = 0.6
epsilon2 = 0.1
episode_goal = []
total_action: set = {0, 1, 2, 3}

q_table = np.zeros([100, 4])
no_episodes = 10000000
times_goal = 0
times_traped = 0

print("now lets test the policy we have created")

list_steps = []
goal_steps = []
trap_steps = []
test_goal = 0

q_table = np.array([[ -9.92753319  -9.96334075  -9.94865709  -9.92937029],
         [ -9.94313795 -10.6367657   -9.92829533 -10.08697854],
         [-10.35177014 -91.02150903 -11.18930903 -27.91764469],
         [ -9.91115463 -10.14600227  -9.90429157  -9.90852042],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [-10.55758386 -11.57411751 -94.30547127 -10.45268284],
         [-17.7158331  -99.34813936 -28.38004011 -19.51097129],
         [-11.82988638 -99.78791682 -10.26481527 -17.04840509],
         [-10.45202515 -11.39163579 -10.4815545  -10.19330297],
         [ -9.92914059  -9.99459198 -10.00116994 -10.03918604],
         [-10.95514385 -11.12904235 -21.50560259 -99.98886692],
         [  0.           0.           0.           0.        ],
         [-10.33262205  -8.07856208 -99.90268183 -93.7335213 ],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [-12.46706575 -12.04334815 -10.37368386 -96.35950415],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [-11.76728361 -11.89714519 -99.5042005  -20.50606985],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [ -8.29915705  -3.90477808  -5.20655156  -4.65982887],
         [-75.25036788 -29.56737338  -4.85961957 -18.1866784 ],
         [ -2.86509493  -2.43481478  -3.16539023  -7.52271361],
         [-10.92321199  -8.43399754 -10.85944135 -97.97611908],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [-23.03741423 -17.26970755 -99.58125311 -13.54395142],
         [-10.02992994 -10.30236507 -10.02738826 -10.49392934],
         [-11.04577207 -98.75895548 -24.42270436 -11.5248266 ],
         [  0.           0.           0.           0.        ],
         [ -3.38208792  -2.21056104  -2.46080313  -1.86383401],
         [ -4.11114141  -0.58776805  -2.24345568  -2.41846972],
         [ -2.15445057  -2.49128961  -2.07679375  -6.15553428],
         [ -8.51776891 -98.92462008  -9.91091186 -16.76428057],
         [-99.51728371 -96.3964419  -13.99817548 -37.35963053],
         [-98.69307869 -13.30388888 -19.33100684 -11.02725709],
         [-12.65588633 -10.04882804 -11.57456609 -10.06593817],
         [-10.0331281  -27.23399575 -14.8495471  -88.26408416],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [-25.94248574  -6.08082818 -95.2069265   -0.72491822],
         [ -1.67298328   1.1269281   -2.35515225  -1.31812067],
         [ -1.5335735   -0.64211281  -0.9784425  -94.02554909],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [ -9.92411509  -9.37888215  -9.41033203 -10.68710122],
         [-99.46194208 -91.29359122 -21.15653871 -10.63108748],
         [-99.52913623 -34.04969289 -10.64307534 -13.11475369],
         [ -3.11124341 -71.91533539  -9.94246726   0.50211074],
         [ -0.62964786   3.23935494  -0.95281274   0.87505072],
         [  0.59429571   3.98896768   0.85857271   0.92695894],
         [  0.           0.           0.           0.        ],
         [-99.98449734  11.78744636   9.8462741  -81.69022717],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [ -9.3760993  -15.61254025  -9.83650391 -88.21418545],
         [  0.           0.           0.           0.        ],
         [-27.55692658 -18.78524074 -99.02171248 -94.11037027],
         [  0.           0.           0.           0.        ],
         [-11.8291706    0.6674644  -89.47155983   5.54266486],
         [  1.51427729   0.13004768   2.39438136   7.6120073 ],
         [  5.55763708 -84.24190244   3.99521301   9.40993675],
         [ 11.30721633   8.06269284   8.66542342  12.19593321],
         [-91.21282429 -98.8946631   12.27295076  15.814271  ],
         [-92.46832534  12.57046627   2.55234645  27.20785307],
         [ -9.37006949 -10.42501599 -10.44638467 -10.34291609],
         [-85.92733921 -13.01869998  -9.58350333 -15.67323837],
         [-53.53342178 -99.99999899 -15.83610537 -99.82491929],
         [  0.           0.           0.           0.        ],
         [  2.67960353  -0.48589349 -97.95575237 -14.23264715],
         [ -0.32417062  -7.72698588   1.07858114 -99.80381504],
         [  0.           0.           0.           0.        ],
         [ 10.22718974 -99.99109602 -99.98783185 -96.29285977],
         [  0.           0.           0.           0.        ],
         [ 37.40017354  21.41408845 -45.39875283  22.17996067],
         [-21.74787281 -99.913769   -28.34789034 -13.96940883],
         [-28.65058532 -21.46035743 -11.05416438 -95.83732089],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [  0.74423541  -1.66359469  -1.21451284  -1.11418837],
         [ -0.11214718  -1.35571611  -1.69293235  -4.39146142],
         [-99.88783245 -94.93292073  -1.96239138 -99.92672855],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [ 71.15002539  99.3328075   87.9689738   86.05127951],
         [  0.           0.           0.           0.        ],
         [-16.30126339 -30.8444932  -80.76125711 -17.20184979],
         [-71.26993514 -17.41251768 -18.14341115 -13.33695316],
         [  0.           0.           0.           0.        ],
         [ -1.39488885  -1.79805364  -2.66501951  -1.70551234],
         [ -1.22023303  -3.49665773  -4.68087982 -87.03592892],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ],
         [  0.           0.           0.           0.        ]])

for episodes in range(0, 100):

    # print("episode number %i running" % episodes)
    terminal_state = 0
    steps = 0
    env.reset()
    position = env.position
    print((position[0]+1,position[1]+1))
    state = ((list(position)[0] * 10) + list(position)[1])

    while True:
        chosen_action = np.argmax(q_table[state, :], axis=0)
        if random.uniform(0, 1) < epsilon2:
            actual_action = random.choice(list(total_action - {chosen_action}))
        else:
            actual_action = chosen_action

        # taking action
        # if action == 0:
        #     print("up")
        # if action == 1:
        #     print("down")
        # if action == 2:
        #     print("left")
        # if action == 3:
        #     print("right")

        list_next_state = env.step(actual_action)
        steps = steps + 1
        if list_next_state[3]:
            print("goal reached")
            #print(steps)
            test_goal = test_goal + 1
        if (list_next_state[-1] and (not list_next_state[3])):
            #print("trapped")
            times_traped = times_traped + 1

        position = copy.copy(list_next_state[0])
        print(position)
        terminal_state = list_next_state[-1]
        next_state = ((list(position)[0] * 10) + list(position)[1])

        # # Recalculate
        # q_value = q_table[state, action]
        # max_value = np.max(q_table[next_state, :])
        # new_q_value = (1 - learning_rate) * q_value + learning_rate * (reward + discount_rate * max_value)
        # q_table[state, action] = new_q_value
        state = next_state

        if terminal_state:
            break
    print("while loop exited--------------------------")
print(test_goal)

